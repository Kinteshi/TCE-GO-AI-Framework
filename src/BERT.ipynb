{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\r\n","from collections import defaultdict\r\n","from datetime import datetime\r\n","import time\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import pandas as pd\r\n","import seaborn as sns\r\n","import torch\r\n","import torch.nn.functional as F\r\n","from sklearn.metrics import classification_report, f1_score\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from torch import nn, optim\r\n","from torch.utils.data import DataLoader, Dataset\r\n","from transformers import (AdamW, BertModel, BertTokenizer,\r\n","                          get_linear_schedule_with_warmup)\r\n","\r\n","from preprocessing.dataprep import filter_tce_data, text_preprocessing\r\n","from preprocessing.text import fixColumnName\r\n","\r\n","sns.set()\r\n","\r\n","RANDOM_SEED = 15\r\n","PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\r\n","MAX_LEN = 156\r\n","BATCH_SIZE = 32\r\n","EPOCHS = 15\r\n","\r\n","np.random.seed(RANDOM_SEED)\r\n","torch.manual_seed(RANDOM_SEED)\r\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TCEDataset(Dataset):\r\n","    def __init__(self, empenho, targets, tokenizer, max_len):\r\n","        self.empenho = empenho\r\n","        self.targets = targets\r\n","        self.tokenizer = tokenizer\r\n","        self.max_len = max_len\r\n","\r\n","    def __len__(self):\r\n","        return len(self.empenho)\r\n","\r\n","    def __getitem__(self, item):\r\n","        empenho = str(self.empenho[item])\r\n","        target = self.targets[item]\r\n","        encoding = self.tokenizer.encode_plus(\r\n","            empenho,\r\n","            add_special_tokens=True,\r\n","            max_length=self.max_len,\r\n","            return_token_type_ids=False,\r\n","            padding='max_length',\r\n","            return_attention_mask=True,\r\n","            return_tensors='pt',\r\n","            truncation=True\r\n","        )\r\n","        return {\r\n","            'empenho_text': empenho,\r\n","            'input_ids': encoding['input_ids'].flatten(),\r\n","            'attention_mask': encoding['attention_mask'].flatten(),\r\n","            'targets': torch.tensor(target, dtype=torch.long)\r\n","        }\r\n","\r\n","\r\n","def create_data_loader(df, tokenizer, max_len, batch_size):\r\n","    ds = TCEDataset(\r\n","        empenho=df.empenho.to_numpy(),\r\n","        targets=df.encodedNatureza.to_numpy(),\r\n","        tokenizer=tokenizer,\r\n","        max_len=max_len\r\n","    )\r\n","    return DataLoader(\r\n","        ds,\r\n","        batch_size=batch_size,\r\n","    )\r\n","\r\n","\r\n","class NaturezaClassifier(nn.Module):\r\n","    def __init__(self, n_classes):\r\n","        super(NaturezaClassifier, self).__init__()\r\n","        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n","        self.drop = nn.Dropout(p=0.3)\r\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n","\r\n","    def forward(self, input_ids, attention_mask):\r\n","        bert_output = self.bert(\r\n","            input_ids=input_ids,\r\n","            attention_mask=attention_mask\r\n","        )\r\n","\r\n","        output = self.drop(bert_output['pooler_output'])\r\n","        return self.out(output)\r\n","\r\n","\r\n","def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\r\n","    model = model.train()\r\n","    losses = []\r\n","    correct_predictions = 0\r\n","    predictions = []\r\n","    real_values = []\r\n","    for d in data_loader:\r\n","        input_ids = d[\"input_ids\"].to(device)\r\n","        attention_mask = d[\"attention_mask\"].to(device)\r\n","        targets = d[\"targets\"].to(device)\r\n","        outputs = model(\r\n","            input_ids=input_ids,\r\n","            attention_mask=attention_mask\r\n","        )\r\n","        _, preds = torch.max(outputs, dim=1)\r\n","        loss = loss_fn(outputs, targets)\r\n","        correct_predictions += torch.sum(preds == targets)\r\n","        losses.append(loss.item())\r\n","        loss.backward()\r\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\r\n","        optimizer.step()\r\n","        scheduler.step()\r\n","        optimizer.zero_grad()\r\n","        predictions.extend(preds)\r\n","        real_values.extend(targets)\r\n","    predictions = torch.stack(predictions).cpu()\r\n","    real_values = torch.stack(real_values).cpu()\r\n","    macro = f1_score(real_values, predictions, average='macro')\r\n","    micro = f1_score(real_values, predictions, average='micro')\r\n","    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\r\n","\r\n","\r\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\r\n","    model = model.eval()\r\n","    losses = []\r\n","    correct_predictions = 0\r\n","    predictions = []\r\n","    real_values = []\r\n","    with torch.no_grad():\r\n","        for d in data_loader:\r\n","            input_ids = d[\"input_ids\"].to(device)\r\n","            attention_mask = d[\"attention_mask\"].to(device)\r\n","            targets = d[\"targets\"].to(device)\r\n","            outputs = model(\r\n","                input_ids=input_ids,\r\n","                attention_mask=attention_mask\r\n","            )\r\n","            _, preds = torch.max(outputs, dim=1)\r\n","            loss = loss_fn(outputs, targets)\r\n","            correct_predictions += torch.sum(preds == targets)\r\n","            losses.append(loss.item())\r\n","            predictions.extend(preds)\r\n","            real_values.extend(targets)\r\n","    predictions = torch.stack(predictions).cpu()\r\n","    real_values = torch.stack(real_values).cpu()\r\n","    macro = f1_score(real_values, predictions, average='macro')\r\n","    micro = f1_score(real_values, predictions, average='micro')\r\n","    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["warnings.filterwarnings('ignore')\r\n","\r\n","start_time = time.time()\r\n","start_date = datetime.now()\r\n","print(f'Starting evaluation at {start_date.strftime(\"%d/%m/%Y %H:%M:%S\")}')\r\n","print()\r\n","print('Loading data...')\r\n","\r\n","data_loading_time = time.time()\r\n","data = pd.read_csv('../database/dadosTCE.csv',\r\n","                   low_memory=False, encoding='utf-8')\r\n","print(f'Data loading time: {(time.time() - data_loading_time):.2f}s')\r\n","print()\r\n","\r\n","data_prep_time = time.time()\r\n","print('Preprocessing data...')\r\n","\r\n","# data = data.sample(1000,).reset_index(drop=True)\r\n","data.columns = list(map(fixColumnName, data.columns))\r\n","data, _ = filter_tce_data(data, '../database/norel.xlsx')\r\n","\r\n","\r\n","df = data[['empenho_historico', 'natureza_despesa_cod']]\r\n","del data\r\n","df.columns = ['empenho', 'natureza']\r\n","df.empenho = df.empenho.apply(text_preprocessing)\r\n","\r\n","\r\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n","\r\n","df_train, df_test = train_test_split(\r\n","    df,\r\n","    test_size=0.3,\r\n","    random_state=RANDOM_SEED,\r\n","    stratify=df.natureza\r\n",")\r\n","\r\n","\r\n","lb = LabelEncoder()\r\n","df_train['encodedNatureza'] = lb.fit_transform(df_train.natureza)\r\n","df_test['encodedNatureza'] = lb.transform(df_test.natureza)\r\n","\r\n","np.save('classes.npy', lb.classes_)\r\n","\r\n","# df_val, df_test = train_test_split(\r\n","#     df_test,\r\n","#     test_size=0.5,\r\n","#     random_state=RANDOM_SEED,\r\n","#     stratify=df_test.natureza\r\n","# )\r\n","\r\n","train_data_loader = create_data_loader(\r\n","    df_train, tokenizer, MAX_LEN, BATCH_SIZE)\r\n","# val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\r\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\r\n","\r\n","print(f'Data preparation time: {(time.time() - data_prep_time):.2f}s')\r\n","# print(f'Training shape: {X_train.shape}')\r\n","# print(f'Test shape: {X_test.shape}')\r\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bert_time = time.time()\r\n","print('Training BERT...')\r\n","\r\n","model = NaturezaClassifier(len(lb.classes_))\r\n","model = model.to(device)\r\n","\r\n","# model.load_state_dict(torch.load('best_model_state.bin', map_location=torch.device(device)))\r\n","\r\n","optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\r\n","total_steps = len(train_data_loader) * EPOCHS\r\n","scheduler = get_linear_schedule_with_warmup(\r\n","    optimizer,\r\n","    num_warmup_steps=0,\r\n","    num_training_steps=total_steps\r\n",")\r\n","loss_fn = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = defaultdict(list)\r\n","best_accuracy = 0\r\n","ant_val_loss = 0\r\n","patience = 0\r\n","for epoch in range(EPOCHS):\r\n","    starting = time.time()\r\n","    print(f'Epoch {epoch + 1}/{EPOCHS}')\r\n","    print('-' * 10)\r\n","    train_acc, train_loss, train_macro, train_micro = train_epoch(\r\n","        model,\r\n","        train_data_loader,\r\n","        loss_fn,\r\n","        optimizer,\r\n","        device,\r\n","        scheduler,\r\n","        len(df_train)\r\n","    )\r\n","    print(\r\n","        f'Train loss {train_loss} accuracy {train_acc} macro {train_macro} micro {train_micro}')\r\n","    val_acc, val_loss, val_macro, val_micro = eval_model(\r\n","        model,\r\n","        test_data_loader,\r\n","        loss_fn,\r\n","        device,\r\n","        len(df_test)\r\n","    )\r\n","    print(\r\n","        f'Val loss {val_loss} accuracy {val_acc} macro {val_macro} micro {val_micro}')\r\n","    print()\r\n","    history['train_acc'].append(train_acc)\r\n","    history['train_loss'].append(train_loss)\r\n","    history['train_macro'].append(train_macro)\r\n","    history['train_micro'].append(train_micro)\r\n","    history['val_acc'].append(val_acc)\r\n","    history['val_loss'].append(val_loss)\r\n","    history['val_macro'].append(val_macro)\r\n","    history['val_micro'].append(val_micro)\r\n","    if val_acc > best_accuracy:\r\n","        torch.save(model.state_dict(), 'best_model_state.bin')\r\n","        best_accuracy = val_acc\r\n","    \r\n","    print(f'Epoch time: {(time.time()-starting)/60}')\r\n","\r\n","print()\r\n","print(f'BERT training time: {(time.time() - bert_time):.2f}s')\r\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\r\n","plt.plot(history['train_acc'], label='train accuracy')\r\n","plt.plot(history['val_acc'], label='validation accuracy')\r\n","plt.title('Training history')\r\n","plt.ylabel('Accuracy')\r\n","plt.xlabel('Epoch')\r\n","plt.legend()\r\n","plt.ylim([0, 1])\r\n","plt.savefig('Acc.png')\r\n","\r\n","plt.cla()\r\n","plt.plot(history['train_loss'], label='train loss')\r\n","plt.plot(history['val_loss'], label='validation loss')\r\n","plt.title('Training history')\r\n","plt.ylabel('Loss')\r\n","plt.xlabel('Epoch')\r\n","plt.legend()\r\n","plt.ylim([0, 1])\r\n","plt.savefig('Loss.png')\r\n","\r\n","plt.cla()\r\n","plt.plot(history['train_macro'], label='train macro')\r\n","plt.plot(history['val_macro'], label='validation macro')\r\n","plt.title('Training history')\r\n","plt.ylabel('Macro')\r\n","plt.xlabel('Epoch')\r\n","plt.legend()\r\n","plt.ylim([0, 1])\r\n","plt.savefig('Macro.png')\r\n","\r\n","plt.cla()\r\n","plt.plot(history['train_micro'], label='train micro')\r\n","plt.plot(history['val_micro'], label='validation micro')\r\n","plt.title('Training history')\r\n","plt.ylabel('Micro')\r\n","plt.xlabel('Epoch')\r\n","plt.legend()\r\n","plt.ylim([0, 1])\r\n","plt.savefig('Micro.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test_acc, _, _, _ = eval_model(\r\n","#     model,\r\n","#     test_data_loader,\r\n","#     loss_fn,\r\n","#     device,\r\n","#     len(df_test)\r\n","# )\r\n","# print(test_acc.item())\r\n","\r\n","\r\n","def get_predictions(model, data_loader):\r\n","    model = model.eval()\r\n","    review_texts = []\r\n","    predictions = []\r\n","    prediction_probs = []\r\n","    real_values = []\r\n","    with torch.no_grad():\r\n","        for d in data_loader:\r\n","            texts = d[\"empenho_text\"]\r\n","            input_ids = d[\"input_ids\"].to(device)\r\n","            attention_mask = d[\"attention_mask\"].to(device)\r\n","            targets = d[\"targets\"].to(device)\r\n","            outputs = model(\r\n","                input_ids=input_ids,\r\n","                attention_mask=attention_mask\r\n","            )\r\n","            _, preds = torch.max(outputs, dim=1)\r\n","            review_texts.extend(texts)\r\n","            predictions.extend(preds)\r\n","            prediction_probs.extend(outputs)\r\n","            real_values.extend(targets)\r\n","    predictions = torch.stack(predictions).cpu()\r\n","    prediction_probs = torch.stack(prediction_probs).cpu()\r\n","    real_values = torch.stack(real_values).cpu()\r\n","    return review_texts, predictions, prediction_probs, real_values\r\n","\r\n","\r\n","bert_time = time.time()\r\n","y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\r\n","    model,\r\n","    test_data_loader\r\n",")\r\n","\r\n","print('Test Classification Report')\r\n","print(classification_report(y_test, y_pred))\r\n","\r\n","print(f'BERT predict time: {(time.time() - bert_time):.2f}s')\r\n","print()\r\n","\r\n","finish_date = datetime.now()\r\n","print(f'Finishing evaluation at {finish_date.strftime(\"%d/%m/%Y %H:%M:%S\")}')"]}],"metadata":{"language_info":{},"orig_nbformat":3},"nbformat":4,"nbformat_minor":2}