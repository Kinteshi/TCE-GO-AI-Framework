{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","from collections import defaultdict\n","from datetime import datetime\n","import time\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn.functional as F\n","from sklearn.metrics import classification_report, f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import (AdamW, BertModel, BertTokenizer,\n","                          get_linear_schedule_with_warmup)\n","\n","from preprocessing.dataprep import filter_tce_data, text_preprocessing\n","from preprocessing.text import fixColumnName\n","\n","sns.set()\n","\n","RANDOM_SEED = 15\n","PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n","MAX_LEN = 156\n","BATCH_SIZE = 32\n","EPOCHS = 15\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class TCEDataset(Dataset):\n","    def __init__(self, empenho, targets, tokenizer, max_len):\n","        self.empenho = empenho\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.empenho)\n","\n","    def __getitem__(self, item):\n","        empenho = str(self.empenho[item])\n","        target = self.targets[item]\n","        encoding = self.tokenizer.encode_plus(\n","            empenho,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","        return {\n","            'empenho_text': empenho,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(target, dtype=torch.long)\n","        }\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","    ds = TCEDataset(\n","        empenho=df.empenho.to_numpy(),\n","        targets=df.encodedNatureza.to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","    )\n","\n","\n","class NaturezaClassifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(NaturezaClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        bert_output = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","\n","        output = self.drop(bert_output['pooler_output'])\n","        return self.out(output)\n","\n","\n","def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","    predictions = []\n","    real_values = []\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        targets = d[\"targets\"].to(device)\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, targets)\n","        correct_predictions += torch.sum(preds == targets)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        predictions.extend(preds)\n","        real_values.extend(targets)\n","    predictions = torch.stack(predictions).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    macro = f1_score(real_values, predictions, average='macro')\n","    micro = f1_score(real_values, predictions, average='micro')\n","    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\n","\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model = model.eval()\n","    losses = []\n","    correct_predictions = 0\n","    predictions = []\n","    real_values = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","            loss = loss_fn(outputs, targets)\n","            correct_predictions += torch.sum(preds == targets)\n","            losses.append(loss.item())\n","            predictions.extend(preds)\n","            real_values.extend(targets)\n","    predictions = torch.stack(predictions).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    macro = f1_score(real_values, predictions, average='macro')\n","    micro = f1_score(real_values, predictions, average='micro')\n","    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","warnings.filterwarnings('ignore')\n","\n","start_time = time.time()\n","start_date = datetime.now()\n","print(f'Starting evaluation at {start_date.strftime(\"%d/%m/%Y %H:%M:%S\")}')\n","print()\n","print('Loading data...')\n","\n","data_loading_time = time.time()\n","data = pd.read_csv('../database/dadosTCE.csv',\n","                   low_memory=False, encoding='utf-8')\n","print(f'Data loading time: {(time.time() - data_loading_time):.2f}s')\n","print()\n","\n","data_prep_time = time.time()\n","print('Preprocessing data...')\n","\n","# data = data.sample(1000,).reset_index(drop=True)\n","data.columns = list(map(fixColumnName, data.columns))\n","data, _ = filter_tce_data(data, '../database/norel.xlsx')\n","\n","\n","df = data[['empenho_historico', 'natureza_despesa_cod']]\n","del data\n","df.columns = ['empenho', 'natureza']\n","df.empenho = df.empenho.apply(text_preprocessing)\n","\n","\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","df_train, df_test = train_test_split(\n","    df,\n","    test_size=0.3,\n","    random_state=RANDOM_SEED,\n","    stratify=df.natureza\n",")\n","\n","\n","lb = LabelEncoder()\n","df_train['encodedNatureza'] = lb.fit_transform(df_train.natureza)\n","df_test['encodedNatureza'] = lb.transform(df_test.natureza)\n","\n","np.save('classes.npy', lb.classes_)\n","\n","# df_val, df_test = train_test_split(\n","#     df_test,\n","#     test_size=0.5,\n","#     random_state=RANDOM_SEED,\n","#     stratify=df_test.natureza\n","# )\n","\n","train_data_loader = create_data_loader(\n","    df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","# val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n","\n","print(f'Data preparation time: {(time.time() - data_prep_time):.2f}s')\n","# print(f'Training shape: {X_train.shape}')\n","# print(f'Test shape: {X_test.shape}')\n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","bert_time = time.time()\n","print('Training BERT...')\n","\n","model = NaturezaClassifier(len(lb.classes_))\n","model = model.to(device)\n","\n","# model.load_state_dict(torch.load('best_model_state.bin', map_location=torch.device(device)))\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = defaultdict(list)\r\n","best_accuracy = 0\r\n","ant_val_loss = 0\r\n","patience = 0\r\n","# for epoch in range(EPOCHS):\r\n","for epoch in range(30):\r\n","    starting = time.time()\r\n","    print(f'Epoch {epoch + 1}')\r\n","    print('-' * 10)\r\n","    train_acc, train_loss, train_macro, train_micro = train_epoch(\r\n","        model,\r\n","        train_data_loader,\r\n","        loss_fn,\r\n","        optimizer,\r\n","        device,\r\n","        scheduler,\r\n","        len(df_train)\r\n","    )\r\n","    print(\r\n","        f'Train loss {train_loss} accuracy {train_acc} macro {train_macro} micro {train_micro}')\r\n","    val_acc, val_loss, val_macro, val_micro = eval_model(\r\n","        model,\r\n","        test_data_loader,\r\n","        loss_fn,\r\n","        device,\r\n","        len(df_test)\r\n","    )\r\n","    print(\r\n","        f'Val   loss {val_loss} accuracy {val_acc} macro {val_macro} micro {val_micro}')\r\n","    print()\r\n","    history['train_acc'].append(train_acc)\r\n","    history['train_loss'].append(train_loss)\r\n","    history['train_macro'].append(train_macro)\r\n","    history['train_micro'].append(train_micro)\r\n","    history['val_acc'].append(val_acc)\r\n","    history['val_loss'].append(val_loss)\r\n","    history['val_macro'].append(val_macro)\r\n","    history['val_micro'].append(val_micro)\r\n","    if val_acc > best_accuracy:\r\n","        torch.save(model.state_dict(), 'best_model_state.bin')\r\n","        best_accuracy = val_acc\r\n","\r\n","    if abs(val_loss - ant_val_loss) < 10**-5:\r\n","        patience += 1\r\n","        if patience == 5:\r\n","            torch.save(model.state_dict(), 'best_model_state.bin')\r\n","            break\r\n","    else:\r\n","        ant_val_loss = val_loss\r\n","        patience = 0\r\n","    \r\n","    print(f'Epoch time: {(time.time()-starting)/60}')\r\n","\r\n","print()\r\n","print(f'BERT training time: {(time.time() - bert_time):.2f}s')\r\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Acc.png')\n","\n","plt.cla()\n","plt.plot(history['train_loss'], label='train loss')\n","plt.plot(history['val_loss'], label='validation loss')\n","plt.title('Training history')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Loss.png')\n","\n","plt.cla()\n","plt.plot(history['train_macro'], label='train macro')\n","plt.plot(history['val_macro'], label='validation macro')\n","plt.title('Training history')\n","plt.ylabel('Macro')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Macro.png')\n","\n","plt.cla()\n","plt.plot(history['train_micro'], label='train micro')\n","plt.plot(history['val_micro'], label='validation micro')\n","plt.title('Training history')\n","plt.ylabel('Micro')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Micro.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test_acc, _, _, _ = eval_model(\n","#     model,\n","#     test_data_loader,\n","#     loss_fn,\n","#     device,\n","#     len(df_test)\n","# )\n","# print(test_acc.item())\n","\n","\n","def get_predictions(model, data_loader):\n","    model = model.eval()\n","    review_texts = []\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            texts = d[\"empenho_text\"]\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","            review_texts.extend(texts)\n","            predictions.extend(preds)\n","            prediction_probs.extend(outputs)\n","            real_values.extend(targets)\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    return review_texts, predictions, prediction_probs, real_values\n","\n","\n","bert_time = time.time()\n","y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","    model,\n","    test_data_loader\n",")\n","\n","print('Test Classification Report')\n","print(classification_report(y_test, y_pred))\n","\n","print(f'BERT predict time: {(time.time() - bert_time):.2f}s')\n","print()\n","\n","finish_date = datetime.now()\n","print(f'Finishing evaluation at {finish_date.strftime(\"%d/%m/%Y %H:%M:%S\")}')\n"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2},"nbformat":4,"nbformat_minor":2}